----------
2018/05/24
----------

# Attendance

  - Adam Carlson
  - Cody Stevens
  - Odi Iancu

# Agenda

  - Introductions
  - State of the Cluster
  - Network Migration
  - Review Fairshare Policy

# State of the Cluster

## Cluster Stats

  - We Will not review cluster stats. Due to migration efforts, they are
    not effective in depicting usage.

## Storage Stats

  - **As of May 21, 2018**

>   - **Research Data on DEAC**
>       - Total Storage Space = **139.56TB**
>       - Used Space = **108.40TB**
>       - % of Utilized Space = **77.68%**
>       - Days to full = **3000+ Days**
>       - Archive Consumption = **15.6TB**
>       - % of Utilized Archive = **79.00%**

# Network Migration

## Reason

  - Retirement of separate HPC firewall

:\* No more need for physical separation, can now be done logically

:\* Cost savings for Network Team

:\* Time savings for Network Team

:\* Ease of administration for HPC Team

  - Retirement of legacy network hardware

:\* Sole user of outdated switch in datacenter

:\* Last remaining type of switch in use by WFU.

  - Migration away from Public IP 152.17.x VLAN, to 10.x VLAN

:\* No need for public IP addresses on all nodes

:\* Adjacent to Enterprise architecture

## Impact

  - New head nodes

:\* Orion.deac.wfu.edu will be transitioned over on June 11.

:\* Log in directly to new head nodes: Pegasus.deac.wfu.edu and
Gemini.deac.wfu.edu

  - Data path the same
  - Accounts/groups the same

:\* Password expiration/update within 30 days will need to be redone

## Changes

  - Newer SLURM Version - 17.02.10 (from 14.11.8)

:\* Better backfill algorithm

:\* No more "adminGrp"

:\* No more "tengig" constraint

:\* Cgroups instead of cpusets

:\* Will investigate "heterogeneous" feature

  - New OpenMPI

:\* Version 3.1.0 compiled and working

:\* Version 2.1.0 was tested and works.

  - All nodes usNIC enabled

:\* 3x faster communication w/OpenMPI (2.1 & 3.1)

:\* Automatically chosen, can run strictly on TCP if need be via mpirun
options

  - No more world access

:\* Thousands of daily hack attempts on our old head nodes

:\* Plan to support external users...

## Migration Plan

  - New head nodes are up\!

:\* Hospital is already using

:\* Isilon storage path did not allow dual usage

  - Start using **NOW**\!
  - Migration plan will be similar to Torque/SLURM transition

:\* Move nodes over weekly through June 11

:\* Supply/Demand could dictate speed

  - Last remaining 5 year nodes will not move

:\* Will be retired with network hardwardware on June 30th.

## Future user benefits

:\* No more separate passwords

:\* Easier transfer of data

:\* Use of Cloud services

:\* Less administrative overhead

# Review Fairshare Policy

:\* Operational costs are up, available grant funding is down.

:\* Centralized funding is dominant contributor

:\* Causing wild swings in fairshare from a small contributor

:\* Thoughts on how to balance

::\* Raise minimum floor on General Group? Currently locked at 7%.

# General Discussion

  - New DEAC website thoughts?

:\* Replaced at beginning of semester

  - New LSTC server is up and running

:\* Will lifecycle again in two years

  - GitHub usage?

:\* SVN retirement last semester, faculty thoughts?

  - Upcoming Fall Purchase

:\* 27 UCS nodes are over 4 years old

:\* a1a-u2-c5-b\[6-8\], a1a-u2-c6-b\[1-8\], a1a-u2-c7-b\[1-8\],
a1a-u2-c8-b\[1-8\]

:\* 16-core Sandy Bridge architecture, 128GB memory each = 540 cores,
3.375TB memory

[Category:WFU DEAC Repcom
Meetings](Category:WFU_DEAC_Repcom_Meetings "wikilink")
