# Attendance

  - Adam Carlson
  - Brian Pearce
  - Damian Valles
  - Gloria Stickney
  - Greg Cook
  - Richard Williams
  - Natalie Holzwarth
  - Sam Cho
  - Timo Thonhauser
  - James Gaewsky

# Agenda

  - State of the Cluster
  - FY16/FY17 Hardware Purchase
  - Updates: New Storage & SLURM
  - New HEAD-NODES
  - O.S. Upgrade on DEAC
  - School of Business
  - Upcoming Tasks

# State of the Cluster

## YouTube

  - We have a YouTube Channel
  - The idea behind this: Training Videos

:\* Very useful with SLURM

:\* Very userful with new users

  - This will have the flip-class approach
  - **YouTube search: DEAC Cluster**
  - If you have an idea of video(s) for us to have, we are open to
    suggestions

## Cluster Stats

  - **As of May 2, 2016**

>   - Number of Nodes = **271**
>       - IBM Nodes = 194
>           - TORQUE IBM Nodes = 55
>           - SLURM IBM Nodes = 139
>       - UCS Nodes = 72
>           - TORQUE UCS Nodes = 16
>           - SLURM UCS Nodes = 56
>       - GPU Nodes = 5

>   - Number of x86 Cores = **3128**
>   - Number of GPU Cores = **8960**
>   - Total RAM = 24817555920 KB or **24TB**

## Performance Data

  - **Job and Node** numbers last 12 months

> **Average ratios of utilized, idle and repair/testing cluster nodes
> for each month**
>
>   - Avg of Utilized Nodes = 77.96%
>   - Avg of Test/Repair Nodes = 5.87%
>   - Avg of Idle Nodes= 16.16%
>
> <!-- end list -->
>
>   -
>     ![<File:nodesMAY2016.png>‎](nodesMAY2016.png‎
>     "File:nodesMAY2016.png‎")
>
> **Average ratios of running and waiting cluster jobs for each month**
>
>   - Avg of Running Jobs = 49.06%
>   - Avg of Waiting Jobs = 50.92%
>
> <!-- end list -->
>
>   -
>     ![<File:jobsMAY2016.png>](jobsMAY2016.png "File:jobsMAY2016.png")

  - **Core and Memory** numbers last 12 months

> **Average ratios of utilized, idle and repair/test number of cores for
> each month**
>
>   - Max Utilized Avg = 94.46%
>   - Requested Avg = 77.5%
>   - Utilized Avg = 65.73%
>
> <!-- end list -->
>
>   -
>     ![<File:coresMAY2016.png>](coresMAY2016.png
>     "File:coresMAY2016.png")
>
> **Average ratios of utilized and idle of the total memory in cluster
> for each month**
>
>   - Max Utilized Avg = 15.94%
>   - Requested Avg = 28.67%
>   - Utilized Avg = 7.97%
>
> <!-- end list -->
>
>   -
>     ![<File:memoryMAY2016.png>](memoryMAY2016.png
>     "File:memoryMAY2016.png")

# FY16/FY17 Annual Purchase

  - **FY16 Purchase**

<!-- end list -->

  - Big year of contributions: **$94,778**

:\* Not counting ORSP match

  - We are still accepting all kinds of contribution

<!-- end list -->

  - Options that we need to submit for final FY purchase:

:\* **Option \#1:** Nodes + GPU expansion + Data & Archive storage

::\* 1 Chassis, 8 nodes, 1 GPU node, 6 GPU expansion cards

:::\* 384 Cores

:::\* 10,752 GPU Cores

::\* 31TB Data Space expansion

::\* 12TB Archive expansion

::\* **Total = $173,206.40**

:::\* $/Core = $292.73

:::\* $/GPU core = $2.78

:::\* $/GB of storage = $0.64

:\* **Option \#2:** Nodes + GPU expansions, No storage

::\* 2 Chassis, 10 nodes, 1 GPU node, 6 GPU expansion cards

:::\* 472 Cores

:::\* 10,752 GPU Cores

::\* **Total = $176,543.81**

:::\* $/Core = $291.08

:::\* $/GPU core = $2.78

:\* **Option \#3:** Nodes + Single-GPU node, No storage

::\* 2 chassis, 12 nodes, 1 GPU node

:::\* 560 Cores

:::\* 1,536 GPU Cores

::\* **Total = $175,897.59**

:::\* $/Core = $289.95

:::\* $/GPU core = $1.29

:\* **Option \#4:** Option 3 + Data & Archive Storage

::\* 2 chassis, 10 nodes, 1 GPU node

:::\* 472 Cores

:::\* 1,536 GPU Cores

::\* 31TB Data Space expansion

::\* 12TB Archive expansion

::\* **Total = $178,356.18**

:::\* $/Core = $291.08

:::\* $/GPU core = $2.78

:::\* $/GB of storage = $0.64

## Summary

    • Timo
      * Preference to hold off on GPU
      * Wait for GPU to gain in popularity
      * Focus on Compute in the meantime
      * Turn on thin provisioning
      * Storage discussion
        * Timo’s group maintains a small quota by deleting old files

    • Sam
      * Agreed on holding off on GPU
      * New GPU architecture came out in March
        * Supposed to be much more efficient
      * Focus on compute and storage
      * Storage discussion
        * Molecular dynamics take up so much space to keep entire trajectory
        * Easier to use external hard drive to share data versus sending electronically

    • Natalie
      * Good with additional CPU capabilities
      * Understands the storage needs

    • Greg
      * With new faculty coming in
      * Already have storage there
      * Estimate for current consumption versus allocated
      * Plan for storage growth

    • Purchase Decision: Option #4

    GPU desktop pilot:
    • Investigate integrating some of Sam’s GPU desktops into the cluster
    • HPC can help admin

  - **FY17 Purchase**

:\* It has been recommend to us by the Procurement Office make two
purchases each year instead of one large purchase order

:\* We will try to try the split approach next school year.

::\* **Buying more robust servers to run the head-nodes**

:\* We will email out the purchasing items in the Fall

:\* We are considering: *' Memory Nodes*'

::\* Nodes would have close to 1TB of RAM

::\* Would have their own partition in SLURM

::\* Ideal cases for any large / huge data sets

::\* Save resources from the rest of the group

:\* **Continuing to buy HPC nodes**

## Summary

    * Greg likes the idea of such resources available
      * Recommends a lower time limit for memory node usage outside of its partitions
    * Natalie wants more info about the various nodes
    * We would schedule a different partition or queue for such Memory Nodes

# Updates: New Storage & SLURM

  - **Storage**

:\* Just over **109TB of storage space** vs 70TB of old array

:\* New paths:

::\* **/deac/opt** ---\> for our binary files

::\* **/deac/generalGrp/*researchGrp*** ----\> for all general group
users

::\* **/deac/*researchGrp*** ----\> for all contributing research groups

::\* **/isilon** ----\> for hospital users, own storage array

  -
    vs
    :\* /rhel6/opt or /rhel4/opt
    :\* /wfurc*\#*/generalGrp
    :\* /wfurc*\#*/researchGrp
    :\* /wfuhs*\#*/researchGrp

<!-- end list -->

  - **SLURM**

:\* **86.95%** of Chassis have been converted

:\* **May 15, 2016** will be the last day for PBS Torque Jobs

:\* We have created documentation on our Wiki

:\* We have created videos on our YouTube Channel

:\* **Last Minute last week**

::\* Transformed rhel6head1, rhel6head2 & rhel6head3 to **hybrid
PBS-SLURM head-nodes**

::\* Running low in system resources on rhel6head4

## Summary

    * At this point, all the nodes are in the SLURM scheduler
    * Also, all research groups and users should be using the new paths from the new storage array.

# New Head-Nodes

  - **Current State of Head-Nodes**
      - Torque Head-Nodes = 3
      - SLURM Head-Nodes = 1

<!-- end list -->

  - **Hardware Specs**
      - rhel6head1, rhel6head2, rhel6head3: TORQUE Head-Node
          - Cores = 16 each
          - RAM = 96GB each
          - Disk = 146GB each
      - rhel6head 4: SLURM Head-Node
          - Cores = 8
          - RAM = 16GB
          - Disk = 73.4GB

<!-- end list -->

  - **Hardware of the Headnodes**
      - rhel6head1 (bc103bl05): Quarterly Maintained, +5yo
      - rhel6head2 (bc103bl06): Quarterly Maintained, +5yo
      - rhel6head3 (bc103bl09): Quarterly Maintained, +5yo
      - rhel6head4 (bc103bl14): Quarterly Maintained, +5yo

<!-- end list -->

  - **Going Forward:**
      - Instead of having them transformed as SLURM head-nodes ---\>
        Stop paying for maintenance
      - Decommission the old hardware
      - Transform the head-nodes to reside as virtual-systems
      - Reduce number back down to 3 Head-Nodes
      - Rename the head-nodes ----\> Something more practical

## Summary

    * Timo asked about why virtual
        * Explained admin benefits
        * vMotion Head-nodes in case hardware goes bad and not have downtime
    * Shouldn’t notice a difference
        * All users can connect to a single point in which then it will be tokenized to another head-node.
        * This in effect relieve traffic on the head-nodes
        * User will and should be able to connect to specific head-nodes if necessary
        * New head-nodes will have different names from the current ones
    * Would like to be an upgrade to what we have at least
        * HPC team explained the configurations of the new virtual head-nodes
        * They would have to be at least the same hardware configurations of current nodes
        * Possibly buying extra RAM in order to host the new head-nodes

# OS Upgrade on DEAC

  - **Current challenges**

:\* Cannot install latest versions of applications

:\* Latest MPI compilers work best on new OS

:\* There's a cost to our current OS

  - **Proposal**

:\* Move to **CentOS 7** vs RHEL 7

::\* Same commands, same environment

::\* OS is free

::\* Easier to upgrade or install new version of applications

:\* Upgrade **OpenMPI** to the latest version

::\* Version **1.10** vs current 1.6

::\* Better optimization through new MPI libraries ---\> better
bare-metal performance

:\* **Ripple Effect**

::\* Upgrade FFTW libraries

::\* Upgrade SCALAPACK & BLAS libraries

::\* Quad-precision compilers/libs (in case 15 decimal-point values
won't do)

  - **Timetable**

:\* TBD for HPC

  - **We need feedback**

:\* What was the experience as a user when Admins changed OS versions?

:\* Long time for changes to take place? Short turn around?

:\* What was the state of the applications? Recompiled?

:\* Testing head-node available? test nodes?

## Summary

    * Talking about phased approach --- like it was done for SLURM
    * Use a partition or separate defined cluster
    * HPC team does not use RHEL support
        * No need to keep RHEL with GPFS now gone
    * Greg likes the newer package availability
    * Damian is having problems installing new versions of applications due to the our current Operating System
    * GNU compilers are awfully darn good
    * Decision: MOVE FORWARD WITH CENTOS7
        * Was informed they would like to see if happen FAST

# School of Business

  - New Graduate Program: MSBA - **MS Business Analytics**
  - Starts this Summer - July 2016
  - Continuing to hire Faculty for the program
  - 3rd-year projection: **120 students**
  - Need infrastructure: **HPC + Big Data + (Storage)**

:\* Current needs: Just HPC, but Big Data will be needed

:\* They have a space problem

:\* They approached us for help

  - **Big Data Research**

:\* Bright Computing - Logical level to use same equipment solution

:\* Amazon - Amazon has approached us as a cloud solution

:\* Dell - Ready to go out of the box solution

  - **Going forward**

:\* School of Business will begin to contribute to the HPC
infrastructure

## Summary

    * Can they be put on a separate cluster or queue?
    * Possible concerns:
         * Worried about School of Business doesn’t need much, then no not much, no no no, THEY NEED EVERYTHING.
         * Time the HPC administrators would spend supporting them and not HPC operations
         * Keep physical hardware separate, on storage and compute, and keep hours spent separate
    * TO DO: investigate and discuss a separate queue just for the School of Business

# Upcoming Tasks

  - Finish SLURM transition
  - Finish FY16 purchase
  - New HEAD-Nodes
  - Assessment of Bright Computing
  - O.S. Upgrade
  - Decommission of RHEL5 servers

[Category:WFU DEAC Repcom
Meetings](Category:WFU_DEAC_Repcom_Meetings "wikilink")
