----------
2016/10/14
----------

# Attendance

  - Damian Valles
  - Adam Carlson
  - Jane Ridge
  - Allin Cottrell
  - Staci Hepler
  - Greg Cook
  - Natalie Holzwarth

# Agenda

  - State of the Cluster
  - Admin's Laundry List
  - New Funding
  - New HEAD-NODES
  - FY17: Fall 2016 Hardware Purchase
  - Backup/DR Solutions
  - School of Business
  - Upcoming Tasks

# State of the Cluster

  - This part of the agenda will not be covered during the meeting. This
    section is to show the utilization of the cluster in the last 12
    months.

## Cluster Stats

  - **As of Oct 1, 2016**

>   - Number of Compute-Nodes = **291**
>       - IBM Nodes = 194
>       - UCS Nodes = 90
>       - GPU Nodes = 7

>   - Number of x86 Cores = **3,824**
>   - Number of GPU Cores = **12,032**
>   - Total RAM = **26.5TB**

## Performance Data

  - **Job and Node** numbers last 12 months

> **Average ratios of utilized, idle and repair/testing cluster nodes
> for each month**
>
>   - Avg of Utilized Nodes = 72.68% (Dropped – More targets to UCS over
>     IBM Nodes)
>   - Avg of Test/Repair Nodes = 3.96%
>   - Avg of Idle Nodes= 23.36%
>
> <!-- end list -->
>
>   -
>     ![<File:NodesOCT2016.png>‎](NodesOCT2016.png‎
>     "File:NodesOCT2016.png‎")
>
> **Average ratios of running and waiting cluster jobs for each month**
>
>   - Avg of Running Jobs = 39.12%
>   - Avg of Waiting Jobs = 60.88% (Wait times have increased)
>
> <!-- end list -->
>
>   -
>     ![<File:JobsOCT2016.png>](JobsOCT2016.png "File:JobsOCT2016.png")

  - **Core and Memory** numbers last 12 months

> **Average ratios of utilized, idle and repair/test number of cores for
> each month**
>
>   - Max Utilized Avg = 90.34%
>   - Requested Avg = 67.4%
>   - Utilized Avg = 57.41% (Finals/Graduation- research took a break)
>
> <!-- end list -->
>
>   -
>     ![<File:CoresOCT2016.png>](CoresOCT2016.png
>     "File:CoresOCT2016.png")
>
> **Average ratios of utilized and idle of the total memory in cluster
> for each month**
>
>   - Max Utilized Avg = 12.68%
>   - Requested Avg = 30.48%
>   - Utilized Avg = 5.78% (We never use RAM – True definition of HPC)
>
> <!-- end list -->
>
>   -
>     ![<File:MemoryOCT2016.png>](MemoryOCT2016.png
>     "File:MemoryOCT2016.png")

## Storage Stats

  - **As of Oct 4, 2016**

>   - **Research Data on DEAC**
>       - Total Storage Space = **118.56TB**
>       - Used Space = **52.85TB**
>       - % of Utilized Space = **44.57%**
>       - Dedup/Compression Savings = **6.4TB**

  -
    ![<File:Data-storage.png>](Data-storage.png "File:Data-storage.png")

>   - **Archived Data on DEAC**
>       - Total Storage Space = **20TB**
>       - Used Space = **8TB**
>       - % of Utilized Space = **40%**
>       - Dedup/Compression Savings = **1.9TB**

  -
    ![<File:Archive-storage.png>](Archive-storage.png
    "File:Archive-storage.png")

# Admin's Laundry List

  - RHEL 5 System Retirement

:\* SVN

:\* User wiki

:\* DEAC website

:\* LS-Dyna license server

  - SLURM partition priority change

:\* Lower partition multiplier to make equal to
    fairshare

    ----> RHEL 5 Management servers will be discontinued by RedHat by the end of April 2017
    ----> SVN will go away and introduce GIT for the Subversion control in the cluster
    ----> The WIKI will go through a big upgrade and will be hosted on a different server, same URL
    ----> The DEAC site will not be hosted by HPC, it will be moved the I.S. services, same URL
    ----> LS-Dyna license server will have a new operating system, e.g. a new license will be needed
    ----> SLURM:
    *** Changes to the partition weights will be changed to match FAIRSHARE values
    *** Incident over the summer in where user had zero-fairshare and yet took over small-partition
    *** Possible new partition for the older non-infiniband IBM nodes: bigger priority when users do not require performance

# New Funding

  - HPC Received **$278,000**
  - Funding was received due to the feedback received from the **IT
    Planning Faculty Discussion Group** back in **Nov. 2015**
  - Funds do **NOT** expire at the end of the Fiscal Year.

<!-- end list -->

    ----> This is a one-time fund
    ----> We can utilize funds for more hardware (compute/storage/network), software and/or licenses that will be needed
    ----> Need to compile list of items that the funds will be utilized for purchasing

# New Head-Nodes

  - Four head-nodes:
      - **orion**.deac.wfu.edu --\> Round Robin Server
      - **hydra**.deac.wfu.edu
      - **virgo**.deac.wfu.edu
      - **libra**.deac.wfu.edu

<!-- end list -->

  - Currently ready for use in testing, but not going full production
    until new ESX servers in
    place.

<!-- end list -->

    ----> New suggested head-node names: short, simple, and easy to remember
    ----> ORION will be the connection point to the cluster, not necessary a head-node
    ----> Users will have the ability to access an individual head-node when necessary
    ----> Goal: getting rid-off the numerical naming as many connect to #1 head-node

# FY17: Fall 2016 Purchase

Changing to two primary purchases in the year, to lower overhead of one
large purchase. Allows us to purchase immediate needs at beginning of
the year.

  - 6 new Compute Nodes (a1a-u2-c15-b\[3-8\])

:\* 44-core broadwell

:\* 256GB RAM

:\* 250GB /scratch

  - 2 new ESX servers

:\* 44-core broadwell

:\* 512GB RAM

  - **Total: ~$100K**

The new ESX servers will better accommodate our new head nodes.
Additionally, upcoming tasks will grow our virtual environment, and
allow us to replace legacy systems/services.

    ----> Purchase is approved
    ----> The new head-nodes will be in the new ESX servers
    ----> Greg proposed instead of buying more hard drives, just keep buying new head-nodes and fall under new scratch partition as a possible cost savings
    ----> Natalie would like to have a table/diagram of all possible constraints for the cluster.  Damian will document such table/diagram.

# Backup/DR Solutions

  - **NetApp Cloud ONTAP**

:\* Assumptions for 105TBs of data with 30% dedupe/compression ratio

:\* Low Performance: **$25,891.00/year** Monthly GB cost: $0.02

:\* Regular Performance: **$43,260/year** Monthly GB cost: $0.03

  - **Cost of a secondary onsite storage array**

:\* Assuming a secondary FAS8040 unit with equivalent storage to today-
**$42,767.60/year**

  - **Costs of Backup today** (Enterprise budget absorbs costs at this
    time)

:\* PVU Costs: $2,112/year

:\* Tape & Tape Library Support Costs: $25,763/year (tape is $0.17/GB -
I have taken out TSM costs)

:\* Assuming 148TBs of data footprint - today HPC has 148TB footprint
with copy pools so this number is higher than the amount of data in
storage

:\* Total:
    **$27,875/year**

    ----> Since the Enterprise side of I.S. is moving away from TSM, we need to consider our HPC Backup solution
    ----> Jane Ridge, Storage Administrator at Wake, presented current and possible solutions
    ----> The vote:  NetApp Cloud ONTAP on AWS will be implemented
    ----> Time frame: This will be implemented within 1-1.5 years from now

# School of Business

  - The MSBA Program has contributed to the HPC cluster
  - They have approved contribution to the HPC cluster for the next **5
    years**
      - **$19,352.60 / year**
  - Class work at this point

<!-- end list -->

    ---> This is the buy-in for the School of Business
    ---> Their contributions are for own dedicated HPC node(s)
    ---> The will have their own SLURM account/partition, aside from the rest of campus
    ---> The MSBA program will eventually has to part away from HPC nodes and move to Hadoop solution

# Upcoming Tasks

  - Tengig system improvements:

:\* Reduce latency by 25%

:\* GPU testing

  - Hadoop Investigation
  - Active historical job monitoring, cluster usage test pilot
  - Strategic Plan Impacts

:\* Data Center migration (long term Strategic Plan)

:\*
    Governance

    ----> Adam is working with CISCO in order to reduce network latency for UCS "tengig" nodes
    ----> Also working with CISCO in providing us with new firmware for GPU utilization (M6-NVIDIA)
    ----> We received Proof-of-Concept hardware from CISCO to test, we will utilize it for possible Hadoop implementation
    ----> Monitoring program: Graphite, possible programming tool for HPC team and all DEAC users
    ----> Plans to move the DEAC cluster back to campus:
    *** Meetings with the Network team will begin this week
    *** In order to always have a cluster, we will be rolling-transfer equipment from downtown to campus
    *** This will require a second DEAC cluster a we transition
    *** Main goal:  ALWAYS HAVE A RUNNING CLUSTER
    ----> Governance
    *** New governance strategy from I.S. affects Repcom
    *** At some point, we will have a representative to explain the changes

[Category:WFU DEAC Repcom
Meetings](Category:WFU_DEAC_Repcom_Meetings "wikilink")
