
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Cluster Installation &#8212; DEAC Cluster Wiki  documentation</title>
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/language_data.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p><strong>InfiniBand</strong> is a switched fabric communications link used in
high-performance computing and enterprise data centers. Its features
include high throughput, low latency, quality of service and failover,
and it is designed to be scalable. The InfiniBand architecture
specification defines a connection between processor nodes and high
performance I/O nodes such as storage devices.[1] The specification is
maintained by the InfiniBand Trade Association.[2]</p>
<div class="section" id="cluster-installation">
<h1>Cluster Installation<a class="headerlink" href="#cluster-installation" title="Permalink to this headline">¶</a></h1>
<p>The cluster has eight separate pools of InfiniBand nodes, present within
<strong>clan01</strong>, <strong>clan02</strong>, <strong>clan06</strong>, <strong>clan07</strong>, <strong>clan08</strong>, <strong>clan12</strong>,
<strong>clan13</strong>, and <strong>clan14</strong>. All pools are independent of each other; see
<a class="reference external" href="Cluster:Hardware_Configuration#Networking">Cluster:Hardware_Configuration#Networking</a>
for details of the hardware and capabilities.</p>
</div>
<div class="section" id="compiling-mpi-to-use-infiniband">
<h1>Compiling MPI to Use Infiniband<a class="headerlink" href="#compiling-mpi-to-use-infiniband" title="Permalink to this headline">¶</a></h1>
<p>This, as is always the case in compiling, depends on the compiler you
use.</p>
<div class="section" id="gnu">
<h2>GNU<a class="headerlink" href="#gnu" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>We recommend compiling using OpenMPI, which supports both Ethernet
and InfiniBand.</p></li>
</ul>
<p>:* To set up your environment for compiling against OpenMPI, do <code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">load</span> <span class="pre">openmpi-x86_64</span></code>.[3]</p>
<ul class="simple">
<li><p>Also see <a class="reference external" href="Quick_Start_Guide:GNU_OpenMPI">Quick Start Guide:GNU
OpenMPI</a></p></li>
</ul>
</div>
<div class="section" id="intel-mpi">
<h2>Intel MPI<a class="headerlink" href="#intel-mpi" title="Permalink to this headline">¶</a></h2>
<p>Programs compiled with Intel compilers and Intel MPI support multiple
fabrics. No separate library version needs to be used.</p>
<ul class="simple">
<li><p>To use the Intel compiler suite, load the module for the compiler
<code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">load</span> <span class="pre">compilers/intel-2012-lp64</span></code>.[4]</p></li>
<li><p>See also <a class="reference external" href="Quick_Start_Guide:Intel_MPI">Quick Start Guide:Intel
MPI</a></p></li>
</ul>
</div>
</div>
<div class="section" id="running-mpi-programs-on-usnic">
<h1>Running MPI Programs on usNIC<a class="headerlink" href="#running-mpi-programs-on-usnic" title="Permalink to this headline">¶</a></h1>
<div class="section" id="requesting-usnic-nodes">
<h2>Requesting usNIC nodes<a class="headerlink" href="#requesting-usnic-nodes" title="Permalink to this headline">¶</a></h2>
<p>To request using the Infiniband (IB) connectivity in clans where
available, one must specify using the “infiniband” partition in their
<a class="reference external" href="SLURM">SLURM</a> job scripts.</p>
<p><code class="docutils literal notranslate">&#160;&#160; <span class="pre">#SBATCH</span> <span class="pre">--partition=usNIC</span></code></p>
<p>Another important thing to note is that the separate usNIC nodes are
best used within the same fabric (ie, ucs3-##, or ucs2-##). This is
to reduce latency between nodes: one node in another fabric will add an
additional hop, increasing latency. So, in order for your job to
actually use usNIC most efficiently, you must make sure all the
processes in your job run on nodes only within the same fabric. The way
to do this is to use the constraints in your <a class="reference external" href="SLURM">SLURM</a>
job script.</p>
<p>For example:</p>
<p><code class="docutils literal notranslate">&#160;&#160; <span class="pre">#SBATCH</span> <span class="pre">--nodes=14</span></code>
<code class="docutils literal notranslate">&#160;&#160; <span class="pre">#SBATCH</span> <span class="pre">--tasks-per-node=8</span></code>
<code class="docutils literal notranslate">&#160;&#160; <span class="pre">#SBATCH</span> <span class="pre">--cpus-per-task=1</span></code>
<code class="docutils literal notranslate">&#160;&#160; <span class="pre">#SBATCH</span> <span class="pre">--constraints=[ucs-a1a-3]</span></code></p>
<p>This says that the job requires 14 nodes, 8 cores per node, and all of
the nodes must be connected by a single fabric (switches as far as SLURM
are concerned are defined within SLURM’s topology configuration). A
single switch correlates as far as the job is concerned means that all
nodes are contained within the same chassis.</p>
</div>
<div class="section" id="gnu-with-mvapich2">
<h2>GNU with MVAPICH2<a class="headerlink" href="#gnu-with-mvapich2" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>See <a class="reference external" href="Quick_Start_Guide:GNU_OpenMPI">Quick Start Guide:GNU
OpenMPI</a></p></li>
<li><p>The OpenMPI version of mpiexec must be used.</p></li>
</ul>
<p><code class="docutils literal notranslate">&#160;&#160; <span class="pre">#</span> <span class="pre">bash</span> <span class="pre">example</span></code>
<code class="docutils literal notranslate">&#160;&#160; <span class="pre">module</span> <span class="pre">load</span> <span class="pre">openmpi-x86_64</span></code>
<code class="docutils literal notranslate">&#160;&#160; <span class="pre">cd</span> <span class="pre">$SLURM_SUBMITDIR</span></code>
<code class="docutils literal notranslate">&#160;&#160; <span class="pre">np=$(</span> <span class="pre">cat</span> <span class="pre">$SLURM_JOB_NODELIST</span> <span class="pre">|</span> <span class="pre">wc</span> <span class="pre">-l</span> <span class="pre">)</span></code>
<code class="docutils literal notranslate">&#160;&#160; <span class="pre">mpiexec</span> <span class="pre">-n</span> <span class="pre">$np</span> <span class="pre">./my_program</span></code></p>
</div>
<div class="section" id="openmpi">
<h2>OpenMPI<a class="headerlink" href="#openmpi" title="Permalink to this headline">¶</a></h2>
<p>OpenMPI has a simple execution procedure. It will determine number of
nodes, and number of processors per node from the SLURM environment, so
you should not specify that. To run on Infiniband:</p>
<p><code class="docutils literal notranslate">&#160;&#160; <span class="pre">mpirun</span> <span class="pre">--mca</span> <span class="pre">btl</span> <span class="pre">^tcp</span> <span class="pre">my_program</span> <span class="pre">--my-arguments</span> <span class="pre">foo</span> <span class="pre">bar</span></code></p>
<p>N.B. there are two hyphens in “–mca”. “–mca btl ^tcp” says not to use
the tcp (ethernet) BTL component. Technically, this is not required as
OpenMPI should default to the fastest interface available (usNIC).[5]</p>
<p>This execution procedure is independent of what underlying compiler is
used.</p>
<p>You can also specify the <em>usNIC</em> <strong>partition</strong>, which is temporary while
usNIC is being deployed across the cluster.</p>
<p><code class="docutils literal notranslate">&#160;&#160; <span class="pre">#SBATCH</span> <span class="pre">--partition=usNIC</span></code></p>
</div>
</div>
<div class="section" id="references">
<h1>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h1>
<references/><p><a class="reference external" href="Category:Cluster">Category:Cluster</a>
<a class="reference external" href="Category:Hardware">Category:Hardware</a>
<a class="reference external" href="Category:SLURM">Category:SLURM</a></p>
<ol class="simple">
<li><p><a class="reference external" href="http://en.wikipedia.org/wiki/InfiniBand">Wikipedia article on InfiniBand</a></p></li>
<li><p><a class="reference external" href="http://www.infinibandta.org/">InfiniBand Trade Association (IBTA) website</a></p></li>
<li><p><a class="reference external" href="Quick_Start_Guide:Environment_Modules">Quick Start Guide:Environment Modules</a></p></li>
<li><p><a class="reference external" href="http://www.open-mpi.org/faq/?category=tcp#tcp-auto-disable">OpenMPI FAQ: TCP auto-disable</a></p></li>
</ol>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">DEAC Cluster Wiki</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020, Adam Carlson, Cody Stevens, Sean Anderson.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 2.2.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/cluster/usNIC.md.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>