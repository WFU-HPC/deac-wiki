# Attendance

  - Tim will attempt to hold this discussion over email. Should the
    committee request an in-person meeting, these "minutes" will be

# Agenda

  - FY11 Cluster Hardware Purchase

:\* Presentation of Current State

:\* Presentation of Changes

:\* Presentation of New State

# Virtual Discussion

> # Cluster Hardware Purchase
>
> > ## Short, short version
> >
> > :; Replace 28 Compute Nodes
> >
> >   -
> >
> >       -
> >         Remove/retire 8GB blades that have 4 total processing cores
> >         and replace with 48GB blades having 8 total processing
> >         cores.
> >
> > :; Refresh Storage Hardware Device "FASTT1"
> >
> >   -
> >
> >       -
> >         Use WFUHS Enterprise Storage to retire "end of life" storage
> >         device.
> >
> > :; Increase maintainability of storage subsystem
> >
> >   -
> >
> >       -
> >         Purchase IBM v7000 storage device with 24TB of RAW storage
> >
> > :; Refresh Storage Server Blades
> >
> >   -
> >
> >       -
> >         Replace 8 aging storage blades with 10 new, smaller form
> >         factor storage blades
> >
> > :; Summary
> >
> > :::\* As we discussed at previous Rep Comm meetings, we need to
> > increase the "per processor" memory on each blade and the most cost
> > effective means of doing that is replacing the older "dual core"
> > technology blades with the latest "quad core" based blades.
> >
> > :::\* We also have some hardware devices that are approaching the
> > point where they need replacement (such as the storage blades and
> > one storage device). We will leverage some of the free storage from
> > the hospital to assist in this replacement effort.
> >
> > :::\* However, we'll still need some additional storage for
> > projected increases in usage.
> >
> > :::\* We've also encountered support issues with two of our newer
> > storage devices. There are in essence very few controls or commands
> > that can be used to resolve any performance or control issues the
> > devices may be having, leaving us no other option than having to
> > power cycle the storage device. After discussing the product space
> > with IBM, I've identified a potential device (v7000) that could
> > address these issues and at a price point that is not significantly
> > higher than our previous storage devices.
> >
> > ## Current Cluster State
> >
> > See [Cluster:Hardware
> > Configuration](Cluster:Hardware_Configuration "wikilink") for
> > specifics...
> >
> >   - Compute Nodes
> >     We currently have 39 blades with 4 total processing cores (i.e.
> >     dual core, dual processor/socket blades)
> >     :\* Each of these blades has 8GB of physical RAM
> >     We currently have 103 blades with 8 total processing cores (i.e.
> >     quad core, dual processor/socket blades)
> >     :\* 66 of these blades have 16GB of physical RAM
> >     :\* 7 of these blades have 32GB of physical RAM
> >     :\* 30 of these blades have 48GB of physical RAM
> >
> > <!-- end list -->
> >
> >   - Storage
> >     41.9TB of storage online as a GPFS filesystems
> >     675GB of storage held in reserve for /home
> >     5.4TB of storage not in use from Spr 2010 purchase.
> >     15TB "in transit" from the WFUHS Enterprise Storage service.
> >
> > <!-- end list -->
> >
> >   - Storage Nodes
> >     16 Storage Nodes
> >     :\* Two of these nodes are WFUHS purchased nodes for their
> >     storage.
> >
> > ## Final Cluster State
> >
> >   - Compute Nodes
> >     We currently have 11 blades with 4 total processing cores (i.e.
> >     dual core, dual processor/socket blades)
> >     :\* Each of these blades has 8GB of physical RAM
> >     We currently have 131 blades with 8 total processing cores (i.e.
> >     quad core, dual processor/socket blades)
> >     :\* 66 of these blades have 16GB of physical RAM
> >     :\* 7 of these blades have 32GB of physical RAM
> >     :\* 58 of these blades have 48GB of physical RAM
> >
> > <!-- end list -->
> >
> >   - Storage
> >     41.9TB of storage online as a GPFS filesystems
> >     675GB of storage held in reserve for /home
> >     5.4TB of storage not in use from Spr 2010 purchase.
> >     15TB "in transit" from the WFUHS Enterprise Storage service.
> >     18TB (usuable) new storage from Winter 2010 purchase.
> >
> > <!-- end list -->
> >
> >   - Storage Nodes
> >     18 Storage Nodes
> >     :\* 2 nodes dedicated for WFUHS research groups
> >     :\* 2 nodes dedicated to WFURC for WFUHS storage

# Minutes

[Category:WFU DEAC Repcom
Meetings](Category:WFU_DEAC_Repcom_Meetings "wikilink")
