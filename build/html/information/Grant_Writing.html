
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Current Actively Maintained Information &#8212; DEAC Cluster Wiki  documentation</title>
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/language_data.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p><strong>NOTOC</strong> The general idea is that no single description can fit
every grant proposal out there. Below are three relevant descriptions
with various levels of detail on all fronts. Below are further examples
from previous grants, but should not be considered up to date strictly
used beyond context. Please feel free to contact the HPC Team at
<strong><a class="reference external" href="mailto:deac-help&#37;&#52;&#48;wfu&#46;edu">deac-help<span>&#64;</span>wfu<span>&#46;</span>edu</a></strong> with your draft composition if you would like
our editorial advice on its accuracy, suitability, etc.</p>
<ul class="simple">
<li><p><strong>NOTE:</strong> Technical specs (nodes, processors, memory, storage) will
be kept up to date best as possible. To get accurate numbers, please
refer to <a class="reference external" href="Cluster:Hardware_Configuration#Aggregate_Information">Cluster:Hardware Configuration#Aggregate
Information</a>
for generic totals or <a class="reference external" href="Cluster:Hardware_Configuration#Physical_Hardware_Specifications">Cluster:Hardware Configuration#Physical
Hardware
Specifications</a>
for detailed information on current hardware resources.</p></li>
</ul>
<div class="section" id="current-actively-maintained-information">
<h1>Current Actively Maintained Information<a class="headerlink" href="#current-actively-maintained-information" title="Permalink to this headline">¶</a></h1>
<div class="section" id="official-cluster-two-liner">
<h2>Official Cluster Two-Liner<a class="headerlink" href="#official-cluster-two-liner" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>The WFU DEAC Cluster Facility is a centrally managed, University
supported, high performance computing environment with 95
computational nodes containing 3,892 processors, 19TB of total
memory, and 170TB of disk storage jointly funded by faculty research
grant awards and the University. The cluster supports faculty in the
Biology, Biomedical Engineering, Business, Chemistry, Computer
Science, and Physics departments. All capital expenses and grant
contributions will be purchased through the Procurement Uniform
Guidance, and be open-bid to resellers, and added to this
pre-existing infrastructure.</p></li>
</ul>
</div>
<div class="section" id="official-cluster-four-liner">
<h2>Official Cluster Four-Liner<a class="headerlink" href="#official-cluster-four-liner" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>The WFU DEAC Cluster Facility is a high performance computing (HPC)
environment that supports faculty in the Biology, Biomedical
Engineering, Business, Chemistry, Computer Science, and Physics
departments. The facility is a heterogeneous collection of 95 64-bit
Intel Xeon based computational nodes, presenting over 3,892
processors, 19TB of RAM and 170TB of disk storage, with 10-gigabit
low-latency network connectivity. The facility is centrally managed
by two full time systems administrators who received their degrees
and/or certifications through the use of HPC Linux clusters. All
capital expenses and grant contributions will be utilized for
contributing computational hardware to this pre-existing
infrastructure; the computational hardware will be purchased through
the Procurement Uniform Guidance and be open-bid to resellers.</p></li>
</ul>
</div>
<div class="section" id="nih-facility-description">
<h2>NIH Facility Description<a class="headerlink" href="#nih-facility-description" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><p><strong>Computational equipment</strong>:</p>
<p>The DEAC Linux Cluster has 95 computational blades containing over
3,892 processing cores dedicated to research and teaching use. The
facility is a heterogeneous collection of Haswell, Broadwell and
Skylake 64-bit Intel Xeon based computational nodes, presenting over
19TB of RAM and 170TB of disk storage. All of these blades are
contained within a Cisco Unified Computing System (UCS), sharing a
redundant 10G Ethernet fabric; and all of the blades utilize a usNIC
kernel bypass to reduce latency by 67% to improve research that is
highly parallel in nature. The cluster storage environment consists
primarily of a NetApp FAS8040 storage array, hosting over 170TB of
shared storage via NFS; and an unlimited space, cloud-based archive
via NFS from AWS. Each storage path presents it’s storage via
10-gigabit ethernet connectivity from redundant connections; volume
configuration follows best practices to provide the utmost
reliability, availability, and security to users. This arrangement
provides an efficient presentation to our compute nodes, within
minimal resource consumption and administrative overhead. The cluster
also utilizes a virtualization environment with failover redundancy to
provide three head nodes as well as several maintenance node virtual
machines for fault tolerant, efficient cluster operation.</p>
<p>The cluster is directly connected to the Wake Forest University campus
core network router through fiber-based, 10-gigabit Ethernet. Access
to the cluster by users is possible through any one of three
head-nodes. The WFU DEAC cluster is co-located at the A1A facility
within the Innovation Quarter (or Wake Downtown) that includes all
research computing equipment for the University’s Reynolda Campus and
School of Medicine. This facility is approximately 5000 sq-ft with
sufficient cooling and power capacity to sustain all current equipment
(including the WFU DEAC cluster) as well as projected growth.</p>
<p>Internally, the cluster is capable of significant growth. By design,
the cluster scales on all fronts through the simple addition of
hardware: computational power scales by adding compute nodes,
interactive logins scales by adding head nodes, cluster maintenance
requirements scale by adding maintenance nodes, disk bandwidth scales
by adding connectivity to the storage arrays, disk capacity scales by
adding disk trays to the storage arrays.</p>
<p>The current networking infrastructure hardware, based on Cisco’s 6500
series switches, can support this scalability to over 1000 nodes with
gigabit-Ethernet to all nodes. Naturally scaling the “access” layer of
this network, via additional Cisco Catalyst 6500 or Nexus 5000 switch
pairs, would add support for additional nodes.</p>
<p>Wake Forest University and the College of Arts of Sciences provide
administrative support for the DEAC cluster at little cost to the
researchers.</p>
<p>All of the aforementioned cluster hardware is fully supported by the
Wake Forest University Information Services Department and two
full-time dedicated Linux system administrators. All capital expenses
and grant contributions will be utilized for contributing
computational hardware to this pre-existing infrastructure. Through
any Wake Forest University grant contribution, the Office of Research
and Sponsor Programs will match it one-to-one (up to $75K). The
computational hardware will be purchased through the Procurement
Uniform Guidance and be open-bid to resellers.</p>
<p><strong>Major Equipment</strong></p>
<p>N/A, except for the computing equipment described above.</p>
</div></blockquote>
</div>
</div>
<div class="section" id="previous-unmaintained-descriptions">
<h1>Previous Unmaintained Descriptions<a class="headerlink" href="#previous-unmaintained-descriptions" title="Permalink to this headline">¶</a></h1>
<div class="section" id="wfusm-ctsa">
<h2>WFUSM CTSA<a class="headerlink" href="#wfusm-ctsa" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>One page document written for the WFUSM CTSA proposal effort, due by
Fall 2007. This document highlights the centralized support and
scalability of the cluster. These abilities are crucial for the HPC
environment being the solid backbone of any large scale effort.</p></li>
</ul>
<p>:* This has <strong>not</strong> been updated since the Fall 2007 submission.</p>
<blockquote>
<div><p>The WFU DEAC HPC cluster is a research computing environment that is
centrally maintained by the University. The cluster administrator team
currently consists of Timothy Miller and Yonas Abraham, both of whom
have obtained doctoral degrees in Physics using HPC clusters. This
computational experience affords an understanding of user operational
needs and allows for a better and more effective match of cluster
design to the problems being studied.</p>
<p>WFU DEAC provides unique capabilities to campus researchers that are
not available from general campus computing: high speed networking
infrastructure, large scale storage and computational capacity.
Architecturally, WFU DEAC is a Linux-based Beowulf style cluster
consisting of 360 processors with 1 GB RAM per processor. These
processors are distributed among 153 nodes, each with gigabit Ethernet
connectivity. A subset of these nodes uses specialized, high speed,
low latency interconnects: 24 nodes (96 processors) use Infiniband
based standard, 16 nodes (32 processors) use the Myrinet based
technology. Currently, all user accessible nodes (login and
computational) have direct access to 15 TB of usable storage available
through the high performance, parallel filesystem.</p>
<p>The WFU DEAC cluster design allows for a great deal of scalability in
the key areas of storage and computational nodes. We use IBM’s
<em>General Parallel File System</em> (GPFS) software to manage and present
the disk storage connected to the cluster. GPFS provides every key
function that a cluster requires for its data access: fault tolerance,
redundancy, transparent maintenance, scalability in performance
(multiple GB/s), and scalability in capacity (2 PB tested limit). With
current SAN switch hardware, we can grow our storage pool to include
an additional 24~storage controllers, each of which could provided
40-50 TB of storage~(1.0-1.2 PB). Our computational nodes also enjoy
significant growth potential. GPFS has a current limit of 2440 nodes.
Existing switch hardware will allow for an additional 680
computational nodes. The purchase of expansion modules currently
available for these switches would allow an additional 480
computational nodes. Growth beyond these additional 1160 nodes simply
requires the purchase of another networking switch.</p>
<p>As a whole, these technologies allow WFU DEAC cluster researchers to
explore a great many computation problems that exist in research
today. The Myrinet and Infiniband technologies provide the low
latency, high bandwidth communication that is essential for difficult
parallel processing problems (fluid dynamics, thermodynamics, black
hole dynamics). The high performance and large scale filesystem
provides the backbone upon which users can tackle the large data set
problems (Monte Carlo simulations, genetics, bioinformatics,
nuclear/particle physics). Of course, the cluster is also well suited
to ``traditional’’, single processor problems that have no
preconceived parallelization. With multi-core processors and compiler
technology, even traditional software can see some performance gains
using the WFU DEAC cluster and the compilers we license through
compiler vector optimizations.</p>
</div></blockquote>
</div>
<div class="section" id="nsf-mri">
<h2>NSF MRI<a class="headerlink" href="#nsf-mri" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Excerpt that has been adapted for relevance in this forum taken from
the WFU DEAC Rep Com grant proposal for NSF MRI funding in January
2005.</p></li>
</ul>
<p>:<strong>Impact on WFU Research Goals</strong></p>
<blockquote>
<div><p>The most important impact of the WFU DEAC facility is the promotion
and support of cross-department and cross-campus research
collaborations. Over its 5 year existence, the facility has helped to
catalyze tremendous cross-fertilization between research groups. This
initiative has produced a number of collaborations with
experimentalists in the Medical School, the Cancer Biology Center, and
the Structural Biology Center that would be impossible without the
cluster. Through interactions and projects that were initiated partly
as a result of the cluster’s availability, a Structural and
Computational Biophysics Graduate Track has been developed. A
computational chemistry-physics group has evolved and meets monthly to
share research results. Research collaborations with other
institutions, such as UNC-Chapel Hill and USAMRIID, have developed
since the creation of the DEAC cluster. One researcher collaborates
with a faculty member from Winston-Salem State University, a
historically black university, who writes the parallel programs for
use on the DEAC cluster. Finally, collaborations with industry,
including Targacept, ReceptorSolutions, and Nanosonic have developed.
Targacept and ReceptorSolutions have ongoing and pending STTR
proposals to the NIH and the NSF with researchers (including J.
Fetrow, PI, and P. Santago, Investigator) in the departments of
Biomedical Engineering, Physics, and Computer Science.</p>
<p>The Wake Forest University DEAC cluster allows the pursuit of these
types of research projects within the liberal arts curriculum. In this
environment, both graduate and undergraduate students are actively
engaged, working in small groups, directly mentored by faculty. A
unique example of the interdisciplinary use of the DEAC cluster at
this liberal arts university is that of Tim Miller (co-investigator),
Jennifer Burg, and Yue-ling Wang, both in the Computer Science
Department. Together with a local modern dance company, these
researchers use the cluster for an annual multimedia dance
performance. They published a paper entitled “Cluster Computation in
Step with Real-Time Dance: An Experiment in Art/Science Collaboration”
in the 17th International Conference on Parallel and Distributed
Computing (2003) and were invited to do a special presentation at the
conference. Students actively participate in these types of unique
collaborations.</p>
</div></blockquote>
<p>:<strong>Impact on the WFU Educational Mission</strong></p>
<blockquote>
<div><p>The University’s support of a strong IT infrastructure and excellence
in teaching has actively encouraged faculty to innovate with computers
in their teaching. The installation of the DEAC cluster has added
significantly to the resources at our disposal and allows both
undergraduate and graduate students access to state-of-the-art
computing facilities.</p>
<p>From the beginning, the cluster has been organized for use as a
classroom tool, with temporary student accounts, special queues for
course use, and disk space set aside for student- and course-generated
data. Currently 6 courses (Bioinformatics, Physics of Biological
Macromolecules, Parallel Computing, Building Beowulf Clusters, Quantum
Chemistry and Advanced Image Analysis) have implemented use of the
cluster so that students have hands-on experience with parallel
computing in these fields. The cluster has allowed the development of
several new courses, including Building Beowulf Clusters and
Computational Molecular Biophysics Laboratory. The course, Building
Beowulf Clusters, is unique and not offered in many places. Parallel
Computing, draws on the cluster in its curriculum with considerable
success, at least as judged by student comments. The course in quantum
chemistry now provides access to state-of-the-art computational
chemistry tools, allowing students to explore the structure of
molecules in ways previously unavailable.</p>
<p>Faculty members associated with this proposal are teaching graduate
and undergraduate courses that exploit the latest pedagogical theory
and tools. Drs. Fetrow and Salsbury developed and co-teach the
Computational Biophysics Laboratory and complementary lecture course,
Physics of Biological Macromolecules. In these interdisciplinary
courses, students learn and apply the basic principles of protein
structure, thermodynamics, and electrostatics. The lecture alternates
between core material and discussions of crucial papers, highlighting
applications to research problems in protein structure and function.
The laboratory is project-based; students study their assigned protein
and write a research paper presenting and critically reviewing their
results, which is reviewed by other students in the class. The
laboratory part of this course would not exist without access to the
DEAC cluster. This year, the class studied the peroxiredoxin protein
structures, learning how to run various molecular mechanics and
electrostatic calculations (on the cluster) and interpreting those in
terms of protein structure. Undergraduate and graduate students from
the departments of Biochemistry, Biology, Chemistry, and Physics as
well as auditors from the local biopharmaceutical company, Targacept,
attended.</p>
<p>Our teaching efforts impact both women and minorities. Two of the DEAC
cluster major users are women (Fetrow and Holzwarth) and both teach
courses that utilize the cluster. Both have been successful at
mentoring women in the sciences. One graduate student and one research
associate in their current laboratories are women. At another
university, J. S. Fetrow had three women win awards for undergraduate
research under her direction. Drs. Cook, Holzwarth, and Fetrow teach
in the introductory physics courses at Wake Forest and, in that
context, make efforts to recruit students pursue studies in the
sciences, particularly computational physics and biophysics. The
Physics department has had great success attracting women into their
courses. Usually, over half of the students registered for these
introductory courses are women.</p>
</div></blockquote>
<p><a class="reference external" href="Category:FAQ">Category:FAQ</a></p>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">DEAC Cluster Wiki</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020, Adam Carlson, Cody Stevens, Sean Anderson.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 2.2.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/information/Grant_Writing.md.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>