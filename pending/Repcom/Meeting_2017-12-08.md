# Attendance

  - Adam Carlson
  - Cody Stevens
  - Odi Iancu
  - Timo Thonhauser
  - Natalie Holzwarth
  - Greg Cook
  - Jeremy Schap (on behalf of Joel Stitzel/Scott Gayzik)

# Agenda

  - Introductions
  - State of the Cluster
  - Fall Purchase Options
  - Architecture Changes
  - Network Changes
  - Admin's Laundry List

# State of the Cluster

  - This part of the agenda will not be covered during the meeting. This
    section is to show the utilization of the cluster in the last 12
    months.

## Cluster Stats

  - **As of Dec 1, 2017**

>   - Number of Compute-Nodes = ~~295~~ **123**
>       - ~~IBM Nodes = 194~~
>       - UCS Nodes = ~~93~~ 123
>       - GPU Nodes = 7

>   - Number of x86 Cores = ~~**4,088**~~ **3,664**
>   - Number of GPU Cores = **12,032**
>   - Total RAM = ~~**26.5**~~*' 20.6TB*'

## Performance Data

  - **Job and Node** numbers last 12 months

> **Average ratios of utilized, idle and repair/testing cluster nodes
> for each month**
>
>   - Avg of Utilized Nodes = 67.94%
>   - Avg of Test/Repair Nodes = 1.96%
>   - Avg of Idle Nodes= 30.10%
>
> <!-- end list -->
>
>   -
>     ![<File:NodesDEC2017.png>](NodesDEC2017.png
>     "File:NodesDEC2017.png")
>
> **Average ratios of running and waiting cluster jobs for each month**
>
>   - Avg of Running Jobs = 66.77%
>   - Avg of Waiting Jobs = 33.47%
>
> <!-- end list -->
>
>   -
>     ![<File:JobsDEC2017.png>](JobsDEC2017.png "File:JobsDEC2017.png")

  - **Cores** numbers last 12 months

> **Average ratios of utilized, idle and repair/test number of cores for
> each month**
>
>   - Max Utilized Avg = 91.5%
>   - Utilized Avg = 71.96%
>
> <!-- end list -->
>
>   -
>     ![<File:CoresDEC2017.png>](CoresDEC2017.png
>     "File:CoresDEC2017.png")

## Storage Stats

  - **As of Dec 1, 2017**

>   - **Research Data on DEAC**
>       - Total Storage Space = **139.56TB**
>       - Used Space = **107.54TB**
>       - % of Utilized Space = **76.95%**
>       - Days to full = **823 Days**

# Architecture Changes

  - 194 legacy nodes retired

:\* Nodes were between 8-11 years old.

:\* Any issues since? Haven't received any complaints.

  - 4 year lifecycle

:\* Nodes to be retired/replaced before they reach 5 years old.

:\* Keeps cluster modern

:\* Run times more predictable

:\* Easier to maintain

  - Introduction of usNIC

:\* Low latency communication protocol

::\* 15us between nodes via TCP

::\* 5us between nodes via usNIC

:\* Benefits multinode jobs

:\* Test NAMD job :

::\* 1 node, 32 cores: 6 hours, 8 minutes

::\* 4 node, 8 cores: 6 hours, 28 minutes

::\* Only 5% impact across multiple nodes\!

:\* Opens door to multinode, higher core jobs if needed.

  - **Discussion**

:\* No issues were noticed after node retirement

:\* Cluster usage dipped to it's lowest in recent memory before their
retirement

::\* Partially due to users specifying tengig nodes only

::\* This aligned with our decision to retire legacy nodes

::\* The higher usage we see, the less waste and overhead

:\* The 4 year lifecycle plan was understood and overall agreed upon

::\* We do not want to get into the habit of using old nodes again

:\* Understood that usNIC is to be seen as a Bonus offering

::\* It does encourage the upgrade of software to use OpenMPI 2.1 when
possible

::\* Only available on newest nodes currently (a1a-u3-c\[1-4\]-b\[1-8\])

::\* Will be available everywhere after network migration (see below)

# Fall Purchase

  - 29 UCS nodes are over 4 years old

:\* a1a-u2-c2-b\[1-8\], a1a-u2-c3-b\[1-8\], a1a-u2-c4-b\[1-8\],
a1a-u2-c5-b\[1-5\]

:\* 16-core Sandy Bridge architecture, 128GB memory each = 464 cores,
3.625TB memory

  - Replace nodes with 12 new UCS M5 Skylake Nodes

:\* 44-core Skylake architecture, 192GB memory each = 528 cores, 2.25TB
memory

:\* A 64-core net gain

:\* 30% faster memory/cache read/write speeds

:\* Includes 2x 480GB SSDs, for more/faster Scratch space

:\* Cost: **$144K**

  - Future purchases

:\* Fall to retire/replace 4+ year old nodes

:\* Spring purchase to act as plus-up for additional capacity and use
remaining IS/grant/ORSP funding.

  - **Discussion**

:\* The future purchase plan was APPROVED by attendees

:\* The fall purchase was APPROVED

:\* Use of retired nodes for testing was by Greg mentioned below

# Network Changes

  - Retirement of separate HPC firewall

:\* No more need for physical separation, can now be done logically

:\* Cost savings for Network Team

:\* Time savings for Network Team

:\* Ease of administration for HPC Team

  - Migration away from Public IP 152.17.x VLAN, to 10.x VLAN

:\* No need for public IP addresses on all nodes

:\* Adjacent to Enterprise architecture

  - Migration plan will be similar to Torque/SLURM transition

:\* Standup a test bed with small subset of compute nodes

:\* Open up to user's for testing, accessible via new headnode

:\* Once proven 100% functional, cluster downtime to transition

::\* Need good time periods?

:\* Initial end result will be same experience for users.

  - Future user benefits

:\* No separate passwords

:\* Easier transfer of data

:\* Use of Cloud services

:\* Less administrative overhead

  - **Discussion**

:\* Greg suggested the use of retired compute nodes for testbed, rather
than carving out from production cluster resources. This was agreed upon
to be a great idea, and will be done.

:\* Emphasis was put on the fact that after the migration is complete,
there should be no change in operations for users. Perhaps the
Torque/SLURM migration was not the best comparison, as that was a HUGE
change in operation.... only meant in terms of testing before the
transition being done.

# Admin Laundry List

  - New DEAC website coming soon

:\* Need to retire legacy host

:\* No longer on HPC hosted system (less admin overhead)

  - Migration of LSTC Server

:\* Need to retire legacy system

:\* Impacts LS-Dyna Users

:\* Will install new licence on both servers for testing temporarily

  - SVN retirement

:\* Need to retire legacy host

:\* Will provide guidance for using cloud services

::\* Recommend Bit Bucket, steps to convert provided via [Quick Start
Guide:Bit Bucket](Quick_Start_Guide:Bit_Bucket "wikilink")

::\* Github is another popular option

  - **Discussion**

:\* Greg already has migrated to Github after the discussion at our last
Repcom meeting. Voiced how it was very easy to transition and is great
to use for collaboration.

::\* Goal is to retire SVN by the end of January.

:\* There was much talk about the use of cloud resources, and questions
pertaining to efficiency of cost/footprint

::\* Only looking at using cloud services where it makes sense to
improve the quality of service DEAC offers

::\* Goal is to drive down costs and simplify management of the cluster

:\* Talked about reducing administrative overhead

::\* By migrating our network on the backend, we can take advantage of
many Enterprise services currently offered separately.

::\* Will free up our admins to focus on OS upgrades, software
installations, script troubleshooting, training, etc.

::\* Impact should be negligible (or improved) due to these changes.

[Category:WFU DEAC Repcom
Meetings](Category:WFU_DEAC_Repcom_Meetings "wikilink")
