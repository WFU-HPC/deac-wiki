# Attendance

# Agenda

  - User Survey Results
  - Cluster Usage Data
  - FY12 purchase in Winter/Spring
  - ISILON Storage Proof of Concept
  - RHEL6 Upgrade Status

# Discussion

## User Survey Results

*(Results are cut-n-pasted as is. For "satisfaction" questions, the
percentage is based on total number of responses. Raw answer count is
listed after each
> percentage.)*

> ### Please specify the three most significant aspects of the WFU DEAC Cluster that are the most crucial to your regular use of the cluster.
>
>   - First Most Significant
>
> <!-- end list -->
>
> 1.  Available programming languages
> 2.  space on disk
> 3.  Fortran 77
> 4.  Reasonable waiting time in the queue
> 5.  Inifiniband available
> 6.  Speed
> 7.  infiniband (for md simulations)
> 8.  large number of cores for use in parallel computation
> 9.  Centralized data storage
> 10. Availability of compute nodes to run jobs
> 11. Large number of available nodes for running jobs
> 12. It is very well maintained and I don't have to worry about
>     anything.
>
> <!-- end list -->
>
>   - Second Most Significant
>
> <!-- end list -->
>
> 1.  Fair sharing of processor time
> 2.  memory
> 3.  Head nodes to do editing and run short programs
> 4.  Reasonable speed for computer jobs
> 5.  generally short queue times
> 6.  Availability
> 7.  access/cost
> 8.  infiniband
> 9.  Large number of CPUs to handle variable demands
> 10. Having updated compilers and standard libraries
> 11. The cluster is of good size and I can get all my research done.
>
> <!-- end list -->
>
>   - Third Most Significant
>
> <!-- end list -->
>
> 1.  Significant storage space
> 2.  up to date libraries
> 3.  Mathematica
> 4.  Ease of Use
> 5.  good availability
> 6.  Central administration
> 7.  Having help with computing issues; for example help with compiling
>     code and linking to appropriate
> libraries.
>
> ### Please specify the three most significant aspects that are missing in your regular use of the cluster
>
>   - First Most Significant
>
> <!-- end list -->
>
> 1.  None, I just have cyclical projects
> 2.  python not up to date
> 3.  None
> 4.  Queue status sometime doesn't reflect the real job status. Once,
>     status showed the job was running, and the job was actually
>     hanging there.
> 5.  limited availability of some resources during peak usage times
> 6.  SAS program capability (though I know it's coming)
> 7.  infiniband nodes fill up with jobs not running infiniband
> 8.  more infiniband nodes
> 9.  Massive storage space
> 10. An easy way to know if the cluster resources for any given job are
>     being used efficiently.
> 11. Disk space
> 12. The wiki is great, but still need some improvement. I really
>     dislike the fact that you need your password to log in---lots of
>     users use key pairs to log in and don't remember their password
>     (like me).
>
> <!-- end list -->
>
>   - Second Most Significant
>
> <!-- end list -->
>
> 1.  space on disk for DASP updates
> 2.  more infiniband nodes for a single job
> 3.  Support for the latest software
> 4.  Occasionally it has been hard to get a job through the queue,
>     although it is relatively rare.
>
> <!-- end list -->
>
>   - Third Most Significant
>
> <!-- end list -->
>
> 1.  more cores per node
> 2.  It is sometimes hard to compile code and link to installed
>     libraries. While we appreciate Dave's excellent help, perhaps it
>     might be possible to make it easier for
> us?
>
> ### ​Are you satisfied with the overall cluster service you are receiving (hardware resources, resource availability, user support, etc)?
>
>   - Somewhat satisfied 33.3% 4
>   - Completely satisfied 66.7%
> 8
>
> ### ​Are you satisfied with the types of clusters resources made available (Gigabit Ethernet, Infiniband, Types of Storage, Types of Processors, Memory, Intel x86_64 processors, etc)?
>
>   - Somewhat Dissatisfied 8.3% 1
>   - Somewhat Satisfied 25.0% 3
>   - Completely Satisfied 58.3% 7
>   - No Opinion 8.3% 1
>
> ### ​Are you satisfied with the amount of resources available?
>
>   - Somewhat Dissatisfied 8.3% 1
>   - Somewhat Satisfied 41.7% 5
>   - Completely Satisfied 50.0% 6
>
> ### ​Are you satisfied with the user support you receive?
>
>   - Completely Dissatisfied 8.3% 1
>   - Somewhat Satisfied 8.3% 1
>   - Completely Satisfied 83.3% 10
>
> ### ​Do you receive timely responses to your support requests?
>
>   - Completely Dissatisfied 8.3% 1
>   - Somewhat Satisfied 8.3% 1
>   - Completely Satisfied 83.3%
> 10
>
> ### ​Are the resources provided by the HPC team meeting your research needs?
>
>   - Completely Dissatisfied 8.3% 1
>   - Somewhat Satisfied 50.0% 6
>   - Completely Satisfied 41.7% 5

## Cluster Usage Data

  - Summary
    Average Infiniband Job Wait Time = 2 hr
      -
        (see ![<File:2011-12-10> Infiniband Job Wait
        Times.pdf](2011-12-10_Infiniband_Job_Wait_Times.pdf
        "File:2011-12-10 Infiniband Job Wait Times.pdf"))
    Average Ethernet Job Wait Time = 20 mins
      -
        (see ![<File:2011-12-10> Ethernet Job Wait
        Times.pdf](2011-12-10_Ethernet_Job_Wait_Times.pdf
        "File:2011-12-10 Ethernet Job Wait Times.pdf")
    ***Maybe*** some better processor utilization after 48GB nodes were
    added (BC07/BC08)
      -
        (see ![<File:2011-12-10> Processor Utilization Historical
        30mBins.pdf](2011-12-10_Processor_Utilization_Historical_30mBins.pdf
        "File:2011-12-10 Processor Utilization Historical 30mBins.pdf"))
    ***Maybe*** some increase in memory utilization too
      -
        (see ![<File:2011-12-10> Memory Utilization Historical
        30mBins.pdf](2011-12-10_Memory_Utilization_Historical_30mBins.pdf
        "File:2011-12-10 Memory Utilization Historical 30mBins.pdf"))

<!-- end list -->

  - Time Ranges
    entire survey range: 02 Aug 2010 -- 02 Dec 2011
    start of survey to before BC13 was installed (10 Nov 2010)
    between BC13 installation and BC07 installation (01 Mar 2011)
    between BC07 installation and BC14 installation (01 Nov 2011)
    after BC14 installation

## FY12 purchase in Winter/Spring

> ### Drivers in the Decision
>
>   - Suggestions drawn from the survey (for the purchase)
>
> <!-- end list -->
>
> 1.  Disk Space
> 2.  Infiniband
>
> <!-- end list -->
>
>   - Hardware Support
>
> <!-- end list -->
>
> 1.  24-port Infiniband switch (end of support = 12/31/2012)
> 2.  8 of 14 clans are out of support
>
> <!-- end list -->
>
>   - Cluster usage
>
> <!-- end list -->
>
> 1.  Infiniband is somewhat of a resource constraint.
> 2.  Anecdotal indications are that it will continue to be so.
> 3.  Software development is producing more IB capable programs.

  - Cost estimates

<!-- end list -->

1.  Disk Space (24TB raw = $28k, ~18TB usable)
2.  Infiniband (~$18k for each chassis)
3.  One clan of 96GB Nodes (~$60000)

<!-- end list -->

  - Tim's Recommendation

<!-- end list -->

1.  **Storage** (3 Shelves)
2.  **Infiniband** (3 Chassis - replacement for 2 chassis, add one new
    chassis)
3.  **Clan03** (14 blades)
4.  Clan05

<!-- end list -->

  - Infiniband

<!-- end list -->

1.  Re-use 24-port switch to interconnect chassis' to determine need for
    multi-chassis' IB

## Future Hardware Issues

  - Storage Node Remediation

## ISILON Proof of Concept

  - WFBH testing ISILON NAS device
  - Dual connected between WFBH and WFU
  - Performance testing now
  - Workflow testing in the next week or so
  - May opt for much of WFBH provided storage to get migrated to new
    device

## RHEL 6 Upgrade Status

  - Work on the configuration server underway.
  - Pressure is on to get a test environment by February.

[Category:WFU DEAC Repcom
Meetings](Category:WFU_DEAC_Repcom_Meetings "wikilink")---
title: Repcom:Meeting 2012-01-17
permalink: /Repcom:Meeting_2012-01-17/
---

