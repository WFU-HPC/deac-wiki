# Attendance

  - Brian Pearce - IS
  - Adam Carlson - IS
  - Damian Valles - IS
  - Rosey Murton - Procurement
  - Laura Jane Kist - Procurement
  - Natalie Holzwarth - Physics
  - Lucho Dimitrov - Human Genomics
  - Greg Cook - Physics
  - Gloria Stickney - Physics
  - Rebecca Alexander - Chemistry
  - Gloria Muday - Biology

# Agenda

  - State of the Cluster
  - Federal Grants - New Uniform Guidance
  - FY16 Purchase / HPC Storage array
  - Moving forward: New Vision of HPC
  - SLURM
  - Upcoming Tasks

# State of the Cluster

## Twitter

  - We now have a Twitter handle:
    [@WakeHPC](https://twitter.com/WakeHPC)
  - We are using this account for:
      - Another way to broadcast any upcoming maintenance, additions,
        faculty announcements, etc.
      - Connecting with other HPC shops in schools and labs
      - Connecting with HPC hardware/software suppliers

## New DEAC Design

  - We decided that the [website](http://www.deac.wfu.edu) needed more
    flexibility

:\* Multi-device friendly

:\* Auto-fill "Request Account" / "Request Software" pages

  - Please continue to provide (or have students do this) the list of
    publications, thesis, presentations, etc.

## Cluster Stats

  - As of October 19, 2015

>   - Number of Nodes
>       - IBM Nodes = 194
>           - SLURM IBM Nodes = 17
>       - UCS Nodes = 72
>           - SLURM UCS Nodes = 16
>       - GPU Nodes = 5
>       - Total Nodes = **271**

>   - Number of x86 Cores = **3128**
>   - Number of GPU Cores = **8960**
>   - Total RAM = 20227778236 KB or **~2TB**

## File Systems

  - Recently we utilized an EMC-tool to scan our utilization of the
    cluster
  - Key Observations:

>   - 86% of files (46,907 GB) have **not changed in 6+ months**
>   - 82% of files (43,576 GB) have **not changed in 1+ years**
>   - 84% of files (42,819 GB) were **not accessed in 6+ months**
>   - 91% the storage (47,720 GB) was consumed by **files larger than 10
>     MB**

  - File Consumption

![<File:file-consumption.png>](file-consumption.png
"File:file-consumption.png")

  - Number of Files in each File system (5.5TB/file system)

![<File:file-numbers.png>](file-numbers.png "File:file-numbers.png")

## Performance Data

  - **Job and Node** numbers last 12 months

> **Average ratios of utilized, idle and repair/testing cluster nodes
> for each month**
>
>   - Avg of Utilized Nodes = 88.23%
>   - Avg of Test/Repair Nodes = 2.71%
>   - Avg of Idle Nodes= 9.05%
>
> <!-- end list -->
>
>   -
>     ![<File:nodesOCT2015.png>‎](nodesOCT2015.png‎
>     "File:nodesOCT2015.png‎")
>
> **Average ratios of running and waiting cluster jobs for each month**
>
>   - Avg of Running Jobs = 63.89%
>   - Avg of Waiting Jobs = 36.65%
>
> <!-- end list -->
>
>   -
>     ![<File:jobsOCT2015.png>](jobsOCT2015.png "File:jobsOCT2015.png")

  - Data since March 2015 to October 2015 on **Core and Memory** numbers

> **Average ratios of utilized, idle and repair/test number of cores for
> each month**
>
>   - Max Utilized Avg = 100% (Twice - June & July)
>   - Requested Avg = 79.13%
>   - Utilized Avg = 75.26%
>
> <!-- end list -->
>
>   -
>     ![<File:coresOCT2015.png>](coresOCT2015.png
>     "File:coresOCT2015.png")
>
> **Average ratios of utilized and idle of the total memory in cluster
> for each month**
>
>   - Max Utilized Avg = 17.25% (October)
>   - Requested Avg = 26.43%
>   - Utilized Avg = 8.31%
>
> <!-- end list -->
>
>   -
>     ![<File:memoryOCT2015.png>](memoryOCT2015.png
>     "File:memoryOCT2015.png")

# Federal Grants - New Uniform Guidance

  - New rules are applied starting: **7-1-2016**
  - This is what we know so far....

>   - If the a grant was signed **after December 26, 2014**… subject to
>     new audit provisions
>       - Anything prior to that is grandfathered in
>       - Overarching principles still apply, costs must be allowable,
>         allocable, and reasonable

>   - **Charging of Computer Devices**
>       - If costs \<$5K, its allowable for direct costs, then must be
>         essential and allocable
>       - If over $5K, purchase must be classified as capital equipment

>   - **Procurement** (starting next FY)
>
> :\* **\< $3K** - No documentation required for micro purchases
>
> :\* **$3K-$150K** - Evidence of 2 quotes or use of strategic sourced
> vendor
>
> ::\* Which ever is *"most restrictive"* (government or WFU)
>
> ::\* WFU does NOT allow internet quotes
>
> :\* Additional requirements for larger $$$ thresholds

  - WFU will continue to monitor federal activity

:\* Could be changed or delayed before next summer.

  - WFU working to ensure that implementation of the guidance provides
    fast and easy procurement of needed research materials and
    equipment, while ensuring compliance.
  - Need to keep in mind *"restricted vendors"*

# FY16 Annual Purchase

> ## General Approach
>
>   - Primary focus on purchases is core count, will consider higher
>     memory if price is right.
>   - Still maintaining a UCS standard memory configuration of 128GB,
>     the jump can be to 256GB
>
> :\* All UCS nodes are capable of at least 1,536GB - (B200 M4)
>
>   - Because of the Uniform Guidance:
>
> :\* Start applying our purchases starting this FY with Uniform
> Guidance format
>
> :\* Since we have Cisco UCS -\> reach out vendors with RFQs
>
> :\* This will give us past-purchase equipment in the cluster for next
> FY
>
> ## General Funding Status
>
>   - Three grant funding identified so far this FY (THANK YOU\!)
>   - Lori Messer - OSPR will continue to provide cost matching and in
>     the future
>   - Would greatly appreciate any supplemental department, startup, or
>     indirect funding
>   - We have submitted a 5-year budget exercise to IS - Hoping for
>     increase in budget
>
> ## Proposed Configuration
>
>   - Two purchases will be made
>
> :\* **First purchase**: spending contribution from Med School
>
> ::\* 1 Chassis, 5 blades (+ 3 blades already purchased through grants)
>
> :\* **Second purchase**:
>
> ::\* Bundle: Single Chassis + 8 blades
>
>   - Overall:
>
> :\* 2 Chassis, 16 blades total
>
> :\* 512 total cores (Dual Intel E5-2698 v3 processors, 2.30 GHz,
> 16-cores/processor)
>
> :\* This will put us:
>
> ::\* 2028 UCS cores + 1552 IBM cores = **3580 Total Cores**
>
> ::\* **2.2TB** RAM total (128GB/blade on 16 blades) or **2.4TB** RAM
> (256GB/blade)
>
> :\* 300GB/blade
>
> :\* 20GE/blade network connectivity, 40GE/chassis
>
> ::\* Will reduce two original chassis from 40GE/chassis to
> 20GE/chassis
>
> ## New HPC Storage Array Solution
>
>   - Our current storage array is old (tech years) -- **5th year of
>     operation\!\!**
>   - Coming out-of-warranty at the end of December 2015
>   - Started process for new purchase for storage array
>
> :\* Issued Request for Proposal to **EMC, NetApp & Dell**
>
> :\* Had a pre-bid conference call with all vendors
>
> :\* They have submitted questions & we have responded
>
> :\* Vendors have submitted Proposals
>
> :\* Invited for presentations
>
> :\* We expect to make decision on the equipment next week
>
> :\* Turn around time in late-November
>
>   - The equipment will be purchase through a lease-to-own
>
> :\* Mur Muchane (CIO) needs to approve it
>
> :\* Hof Milan might need to approve it
>
>   - Worst case: we would have to extend maintenance cost

# Moving Forward

  - How will Wake look like in 5-10 years from now?

>   - Through President Hatch's address earlier this month, there will
>     be a **push for STEM** for Wake
>   - Through Campus Connections, Dean Michele Gillespie, **Innovation
>     Quarter** is moving forward for stronger collaboration with
>     Hospital
>   - Our point-of-view: How, then, HPC can help complement this
>     mission?
>
> :\* The mission will (should) not change, but the services will have
> to expand in order to reach High Performance Research

  - How will HPC help to leverage the computational needs in the future?

>   - HPC is following a trend that will change in:
>
> :\* Interaction. Web-based interface portals, cloud-component
> utilities. **Cluster for the "99"% approach**.
>
> :\* Data. Less amount of data transfers to perform **HPC+Big
> Data+Visualization** all in the same infrastructure
>
> :\* Code. OpenACC-approach to **run on GPUs** -\> \#pragma or
> \#directive approach to speed up execution.

  - What services will be needed to support ALL research with
    computational components?

>   - No matter what direction, the same computation approach **will
>     stay the same**
>   - **GPUs+Nodes**: Many clusters have a subset of GPU-nodes, we can
>     have a 3-5 year roll out of NVIDIA Nodes
>
> :\* Cisco people are curious in how Wake could use UCS nodes + GPUs

  - What kind of infrastructure will be needed to provide such services?

>   - GPUs or any accelerator?
>   - Composable Infrastructure?
>
> :\* Multi-use or **Reconfigurable** hardware
>
> :\* Visualization or big data or HPC cycles
>
>   - Hybrid cluster: HPC/Hadoop?
>
> :\* Logic-level of doing both **HPC+Big Data** computation
>
> :\* Brightcomputing
>
>   - Need cloud component to HPC?
>
> :\* Mainly **Archive or Backup** solution
>
> :\* Primary storage will still be hosted with cluster
>
>   - **In situ visualization** software?
>
> :\* Data transfers are expensive
>
> :\* No need to move data from cluster and be able to Visualize it with
> same hardware
>
>   - Require **development effort** to reach goals?
>
> :\* We can provide support in code to reach new vision
>
>   - Do we need a **DR solution** as well?
>
> :\* We currently do not have a Disaster Recovery solution
>
> :\* Have a subset of cluster back in campus?

  - **By Utilization Numbers**

>   - **Last 8 months** = 3/4 of the cluster's cores are used on AVERAGE
>   - Roll IBM out NODES -\> Increase UCS hardware for computer & other
>     services as well
>
> :\* We continue to keep the same (and increase) core-count due to
> density

  - **Brainstorming**

>   - Have smaller meetings through the rest of the year
>   - Give opportunity for everyone to bring ideas

# SLURM

![<File:SLURM_repcom_2015.10.27.pdf>](SLURM_repcom_2015.10.27.pdf
"File:SLURM_repcom_2015.10.27.pdf")

# Upcoming Tasks

  - SLURM
  - New storage migration
  - File system migration GPFS to NSF (Lustre File Sytem?)
  - Ganglia
  - Possible SVN replacement

[Category:WFU DEAC Repcom
Meetings](Category:WFU_DEAC_Repcom_Meetings "wikilink")---
title: Repcom:Meeting 2015-10-27
permalink: /Repcom:Meeting_2015-10-27/
---

