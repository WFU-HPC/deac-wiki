# Attendance

  - Conducted via Email

# Agenda

  - New staff\!
  - Cluster Usage Data for RHEL6 (~Summer 2012 until Present)
  - Cluster Annual Purchase for Fiscal Year 2014
  - Current Cluster Configuration
  - Cluster Shutdown for September 2014

# Material from Tim

> ## New Staff\!\!\!
>
> >   - Damian Valles : Joined us in January 2014.
> >     Adam Carlson : Joined us in late-April 2014.
> >
> > <!-- end list -->
> >
> >   - They have already been working on your support requests and have
> >     been doing a great job getting up to speed on the cluster.
> >     Please understand that they are various levels of familiarity
> >     with **our** cluster and our community so some extra grace would
> >     be appreciated. Please make them feel welcome\!
> >   - You can read more information on their backgrounds at
> >     <http://www.deac.wfu.edu/about/staff.html>.
>
> ## Cluster Usage Data for RHEL6
>
> >   - This information is used to inform the research community on how
> >     the cluster is being used as a whole.
> >   - We like to have some information regarding which resources are
> >     constrained (processors or memory) in order to alleviate those
> >     bottlenecks.
> >
> > :\* See [RepCom:Meeting
> > 2010-09-17](RepCom:Meeting_2010-09-17 "wikilink") for examples of
> > information we like to present.
> >
> >   - **Bad News**: The script used for this task was broken in
> >     multiple ways.
> >
> > :\* While I have been able to fix the language specific issues
> > (depracated features, e.g.), I have not had the time to successfully
> > convince myself that the calculations are accurate.
> >
> > :\* Honestly, had I an inkling of the amount of time required, I
> > would have been better off writing it from scratch.
> >
> > :\* **Result**: I do not have any data to present and there is
> > simply no time left to generate it.
> >
> > ### Simplistic TORQUE Reports
> >
> >   - The best I can do is some of the standard TORQUE reporting tools
> >     which only look at CPU requests and walltimes and have no
> >     reporting on memory. You can see those reports here -
> >     <https://wiki.deac.wfu.edu/index.php/Category:WFU_DEAC_Rep_Com_Meetings>
>
> ## Cluster Annual Purchase
>
> >   - **Good News**: The choice of solution from a financial and
> >     performance perspective is very suggestive of a best course of
> >     action.
> >
> > >
> > >
> > > | Option Number | Option Description                    | GB/core | Total CPU-GHz | Price per unit CPU-GHz | Total CPUs | Price per unit CPU | Total Blades | Total Price |
> > > | ------------- | ------------------------------------- | ------- | ------------- | ---------------------- | ---------- | ------------------ | ------------ | ----------- |
> > > | 1             | E5-2697v2 (12core, 2.7GHz), 256GB RAM | 10.67   | 1231.2        | $216.69                | 456        | $585.08            | 19           | 266,794.44  |
> > > | 2             | E5-2660v2 (10core, 2.2GHz), 128GB RAM | 6.4     | 1188          | $197.70                | 540        | $434.94            | 27           | 234,865.83  |
> > > | 3             | E5-2697v2 (12core, 2.7GHz), 128GB RAM | 5.3     | 1231.2        | $211.38                | 456        | $570.74            | 19           | 260,257.11  |
> > > | 4             | E5-2665 (8core, 2.4GHz), 128GB RAM    | 8       | 1036.8        | $269.43                | 432        | $646.64            | 27           | 279,346.87  |
> > >

> > > All options include 10GE, one 300GB 10K SAS drive, 3 years 8x5xNBD
> > > support
> >
> > :\* Options 1 and 2 leverage pre-configured bundles available from
> > Cisco at significant discount levels.
> >
> > :\* Options 3 and 4 are custom configurations and the pricing
> > assumes the same pre-approved discount in options 1 and 2. However,
> > we've been told that almost certainly wouldn't happen.
> >
> > :\* Options 1, 2, and 3 are Ivy Bridge processors - die-shrink
> > version of Sandy Bridge processors (which is option 4)
> >
> > :\* Option 4 is the same configuration we purchased last year.
> >
> > :\* CPU-GHz is merely the number of cores times the clock frequency
> >
> > :\* CPU is number of physical cores (not sockets)
> >
> >   - Option 2, to me, to be the best deal for the funds we have
> >     available.
> >
> > :\* Compared to last year's order (using previous processor
> > generation), we get 15% more processor cycles at 27% less per cycle
> > cost.
> >
> > :\* Compared to the only other attractive option (option 1), we get
> > 4% less processor cycles but by spending 9% less per cycle cost
> > (overall cost savings is only 12%).
> >
> > ::\* Admittedly, a portion of that difference is less memory per
> > node.
> >
> > :\* Compared to option 3 (which compares the 2660v2 against the
> > 2697v2 at the same memory size), we get 4% less processor cycles but
> > by spending 6.5% less per cycle cost (overall cost savings is 10%)
> >
> >   - The only other consideration is that while option 1 has higher
> >     costs at the CPU/blade level, the density allows us to realize
> >     savings on the infrastructure side (2 chassis instead of 3).
> >     That does count for something (about $12k).
> >
> > <!-- end list -->
> >
> >   - Finally, our target available spending amount is about
> >     $250,000.00
> >
> > :\* Other options than \#2 would require additional funding sources
> > or reduction of blades (2 or 3 blades)
> >
> > ## Current Cluster Configurations
> >
> > > For some perspective, here is the current cluster configuration:
> > >
> > > :; Qty 238 - IBM Blades (8 cores each) - 1904 cores total
> > >
> > >   -
> > >
> > >       -
> > >         120 Blades with 16GB RAM
> > >         28 Blades with 48GB RAM
> > >         84 Blades with 96GB RAM
> > >         6 Blades with 144GB RAM
> > >
> > > :; Qty 29 - Cisco B-series blades (16 cores each) - 464 cores
> > > total
> > >
> > >   -
> > >
> > >       -
> > >         Each with 128GB RAM
> > >
> > > <!-- end list -->
> > >
> > >   - Total - 2368 cores (note -The Cisco blades are coming online
> > >     in the next week)
> > >
> > > Our new cluster configuration (with existing Cisco blades online,
> > > and new blades online):
> > >
> > > :; Qty 210 - IBM Blades (8 cores each) - 1680 cores total
> > >
> > >   -
> > >
> > >       -
> > >         92 Blades with 16GB RAM (Retiring 28 old blades)
> > >         28 Blades with 48GB RAM
> > >         84 Blades with 96GB RAM
> > >         6 Blades with 144GB RAM
> > >
> > > :; Qty 56 - Cisco B-series blades (128GB RAM each) - 1004 cores
> > > total
> > >
> > >   -
> > >
> > >       -
> > >         29 Blades with 16 cores
> > >         27 Blades with 20 cores
> > >
> > > <!-- end list -->
> > >
> > >   - Total - 2684 cores
> > >
> > > <!-- end list -->
> > >
> > >   - If the power and cooling loads of the new equipment permit, we
> > >     may be able to leave the 2 chassis of old blades online for an
> > >     additional capacity of 464 cores.
> > >   - However, the older the equipment becomes, the more thought
> > >     that has to be invested into code development, compilation,
> > >     optimizations, etc. In an attempt to reduce operating costs,
> > >     we may be urged to not keep (5 year old nodes) online.
> > >   - Please note: no Infiniband blades are affected by this year's
> > >     purchase plan.
>
> ## Cluster Shutdown in September 2014
>
> >   - Cluster firewalls must be upgraded (annual network refresh). The
> >     entire cluster network will be down for the activity.
> >   - Would like to reserve the entire week of Labor Day (Tuesday
> >     through Sunday) to complete the work.

[Category:WFU DEAC Repcom
Meetings](Category:WFU_DEAC_Repcom_Meetings "wikilink")---
title: Repcom:Meeting 2014-05-14
permalink: /Repcom:Meeting_2014-05-14/
---

